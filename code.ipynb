{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjkb9P_aVbMg",
        "outputId": "d33bbabb-b1a8-46dc-b5f1-31b134e3aba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized Stacked Model Evaluation:\n",
            "Mean Absolute Error: 1.6081753301507995\n",
            "Mean Squared Error: 4.192502738234582\n",
            "R^2 Score: 0.9768340729657486\n",
            "Cross-Validation Mean R^2 Score: 0.9733306236370355\n",
            "Cross-Validation Mean Absolute Error: 1.6035586132961008\n",
            "Cross-Validation Mean Squared Error: 4.924674705375823\n",
            "Optimized submission file saved to: /content/sub6.csv\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import (GradientBoostingRegressor, StackingRegressor,\n",
        "                              ExtraTreesRegressor, RandomForestRegressor)\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load the datasets\n",
        "train_data = pd.read_csv('/content/train.csv')\n",
        "test_data = pd.read_csv('/content/test.csv')\n",
        "sample_submission = pd.read_csv('/content/sample_submission.csv')\n",
        "\n",
        "# Convert the DATE column to datetime and drop the unwanted columns\n",
        "# Unnecessary columns like PRCP (Precipitation), SNWD (Snow Depth), and ELEVATION columns are dropped\n",
        "drop_columns = ['PRCP_A', 'PRCP_B', 'PRCP_C', 'Unnamed: 0', 'ELEVATION_A',\n",
        "                'ELEVATION_B', 'ELEVATION_C', 'SNWD_A', 'SNWD_B', 'SNWD_C']\n",
        "for dataset in [train_data, test_data]:\n",
        "    dataset['DATE'] = pd.to_datetime(dataset['DATE'], dayfirst=True)  # Convert the 'DATE' column to datetime type\n",
        "    dataset.drop(columns=drop_columns, errors='ignore', inplace=True)  # Drop the unnecessary columns\n",
        "\n",
        "# Fill missing temperature values using median and correct TAVG_A\n",
        "# Impute missing values for temperature columns with median values\n",
        "temp_columns = ['TMAX_A', 'TMIN_A', 'TAVG_A', 'TMAX_B', 'TMIN_B',\n",
        "                'TAVG_B', 'TMAX_C', 'TMIN_C', 'TAVG_C']\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "for dataset in [train_data, test_data]:\n",
        "    dataset[temp_columns] = imputer.fit_transform(dataset[temp_columns])  # Apply imputation to temperature columns\n",
        "    dataset['TAVG_A'] = dataset[['TAVG_A', 'TMAX_A']].min(axis=1)  # Ensure TAVG_A is not greater than TMAX_A\n",
        "\n",
        "# Extract date-related features such as year, month, and day from the 'DATE' column\n",
        "def extract_date_features(dataset):\n",
        "    dataset['YEAR'] = dataset['DATE'].dt.year  # Extract the year\n",
        "    dataset['MONTH'] = dataset['DATE'].dt.month  # Extract the month\n",
        "    dataset['DAY'] = dataset['DATE'].dt.day  # Extract the day of the month\n",
        "    dataset['DAYOFWEEK'] = dataset['DATE'].dt.dayofweek  # Extract the day of the week\n",
        "    dataset['WEEKEND'] = (dataset['DAYOFWEEK'] >= 5).astype(int)  # Mark if it's a weekend (1 if true, else 0)\n",
        "    dataset['DAYOFYEAR'] = dataset['DATE'].dt.dayofyear  # Extract the day of the year\n",
        "    dataset['MONTH_DAY'] = dataset['MONTH'] * dataset['DAY']  # Create a feature by multiplying month and day\n",
        "\n",
        "for dataset in [train_data, test_data]:\n",
        "    extract_date_features(dataset)  # Apply the date feature extraction function to both datasets\n",
        "\n",
        "# Define features (X) and target (y) for the training data\n",
        "X = train_data.drop(columns=['DATE', 'TAVG'])  # Drop 'DATE' and target column 'TAVG' from training features\n",
        "y = train_data['TAVG']  # 'TAVG' is the target variable (average temperature)\n",
        "X_test = test_data.drop(columns=['INDEX', 'DATE'])  # Drop 'INDEX' and 'DATE' from the test set\n",
        "\n",
        "# Preprocessing pipeline for numerical and categorical data\n",
        "numeric_features = temp_columns + ['YEAR', 'MONTH', 'DAY', 'DAYOFWEEK', 'WEEKEND', 'DAYOFYEAR', 'MONTH_DAY']\n",
        "categorical_features = []  # No categorical features in this dataset\n",
        "\n",
        "# Preprocessing pipeline for numerical data: imputing missing values, polynomial feature generation, and scaling\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values using the median\n",
        "    ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),  # Generate interaction features\n",
        "    ('scaler', StandardScaler())  # Standardize features\n",
        "])\n",
        "\n",
        "# ColumnTransformer to apply preprocessing to numerical data and any potential categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),  # Apply the numeric_transformer to numeric columns\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)  # Placeholder for categorical features\n",
        "    ])\n",
        "\n",
        "# Apply the transformer to both training and test data\n",
        "X = preprocessor.fit_transform(X)  # Fit and transform the training data\n",
        "X_test = preprocessor.transform(X_test)  # Transform the test data\n",
        "\n",
        "# Splitting the dataset into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)  # 80-20 train-test split\n",
        "\n",
        "# Feature selection using ExtraTreesRegressor's feature importance\n",
        "# ExtraTreesRegressor is used to rank the features based on importance\n",
        "selection_model = ExtraTreesRegressor(random_state=0)\n",
        "selection_model.fit(X_train, y_train)\n",
        "selector = SelectFromModel(selection_model, threshold=\"mean\", prefit=True)  # Select features above the mean importance\n",
        "\n",
        "X_train_selected = selector.transform(X_train)  # Transform the training data using the selected features\n",
        "X_valid_selected = selector.transform(X_valid)  # Transform the validation data\n",
        "X_test_selected = selector.transform(X_test)  # Transform the test data\n",
        "\n",
        "# Define the models with optimized hyperparameters\n",
        "gb_model = GradientBoostingRegressor(\n",
        "    n_estimators=200,  # Number of boosting stages\n",
        "    learning_rate=0.1,  # Learning rate\n",
        "    max_depth=5,  # Maximum depth of the tree\n",
        "    min_samples_split=5,  # Minimum number of samples to split an internal node\n",
        "    min_samples_leaf=3,  # Minimum number of samples to be at a leaf node\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "nn_model = MLPRegressor(\n",
        "    hidden_layer_sizes=(150, 75),  # Two hidden layers with 150 and 75 neurons\n",
        "    activation='relu',  # Activation function for the hidden layers\n",
        "    solver='adam',  # Solver for weight optimization\n",
        "    alpha=0.001,  # L2 penalty (regularization term)\n",
        "    learning_rate_init=0.01,  # Initial learning rate\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "et_model = ExtraTreesRegressor(\n",
        "    n_estimators=300,  # Number of trees in the forest\n",
        "    max_depth=7,  # Maximum depth of the tree\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=3,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=300,  # Number of trees in the forest\n",
        "    max_depth=7,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=3,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "# Create the stacking regressor with the optimized models\n",
        "# The final estimator is Ridge regression\n",
        "stacked_model = StackingRegressor(\n",
        "    estimators=[('gb', gb_model), ('nn', nn_model), ('et', et_model), ('rf', rf_model)],\n",
        "    final_estimator=Ridge(alpha=1.0, random_state=0)\n",
        ")\n",
        "\n",
        "# Train the stacked model\n",
        "stacked_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Evaluate the model on validation data\n",
        "y_pred = stacked_model.predict(X_valid_selected)  # Predict on validation data\n",
        "mae = mean_absolute_error(y_valid, y_pred)  # Calculate mean absolute error\n",
        "mse = mean_squared_error(y_valid, y_pred)  # Calculate mean squared error\n",
        "r2 = r2_score(y_valid, y_pred)  # Calculate R^2 score\n",
        "\n",
        "print(\"Optimized Stacked Model Evaluation:\")\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R^2 Score: {r2}\")\n",
        "\n",
        "# Perform K-fold cross-validation to evaluate the model's stability\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "scores = cross_val_score(stacked_model, X_train_selected, y_train, cv=kf, scoring='r2')\n",
        "\n",
        "print(\"Cross-Validation Mean R^2 Score:\", scores.mean())  # Average R^2 score across folds\n",
        "print(\"Cross-Validation Mean Absolute Error:\", -1 * cross_val_score(stacked_model, X_train_selected, y_train, cv=kf, scoring='neg_mean_absolute_error').mean())\n",
        "print(\"Cross-Validation Mean Squared Error:\", -1 * cross_val_score(stacked_model, X_train_selected, y_train, cv=kf, scoring='neg_mean_squared_error').mean())\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = stacked_model.predict(X_test_selected)  # Predict on the test set\n",
        "\n",
        "# Prepare the submission file\n",
        "submission = sample_submission.copy()\n",
        "submission['TAVG'] = test_predictions  # Add the predicted 'TAVG' to the submission file\n",
        "\n",
        "# Save the submission file\n",
        "submission_file_path = '/content/sub6.csv'\n",
        "submission.to_csv(submission_file_path, index=False)\n",
        "\n",
        "print(\"Optimized submission file saved to:\", submission_file_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
